---
title: "TADA R Package Training: A Markdown for Shepherdstown"
author: "TADA Team"
description: A foray into TADA's major functions for users familiar with R and RStudio.
output: rmarkdown::html_vignette
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  eval = FALSE
)
```


## Welcome!

Thank you for your interest in Tools for Automated Data Analysis (TADA). TADA is an open-source tool set built in the R programming language and available for anyone to download and edit to their specific needs. This [RMarkdown](https://bookdown.org/yihui/rmarkdown/) document walks users through how to download the TADA R package from GitHub, access and parameterize several important functions with a sample dataset, and create basic visualizations. The workflow is similar to a funnel: at each decision point, data that fail QC checks are removed from the core dataset and placed in a separate dataframe, while data that pass are carried into the next step. At the end of the QC checks, the user should be confident that their data are properly documented and applicable to the analysis at hand.

**Note: TADA is still under development. New functionality is added weekly, and sometimes we need to make bug fixes in response to tester and user feedback. We appreciate your interest (and patience) in these helpful tools.**

## Customize or contribute

TADA is housed in a repository on [GitHub](https://github.com/USEPA/TADA). Users desiring to review the base code and customize the package for their own purposes may:

-   Clone the repository using Git

-   Open the repository using GitHub Desktop, or

-   Download a zip file of the repository to their desktop.

Interested in contributing to the TADA package? The TADA team highly encourages input and development from users. Check out the [Contributing](https://usepa.github.io/TADA/articles/CONTRIBUTING.html) page on the TADA GitHub site for guidance on collaboration conventions.

## Install and setup

Users can install the TADA package from GitHub into their R library using the `remotes` package. Copy and paste the code below into your R or RStudio console to download and install.

```{r}
# Install TADA
if(!"remotes"%in%installed.packages()){
  install.packages("remotes")
}
remotes::install_github("USEPA/TADA", ref="develop")
library(TADA)
```

It's that easy! The most stable branch for TADA right now is the develop branch. Contributors generally create their own branches based on develop, make some improvements, and then submit a pull request to be reviewed by the TADA Team. However, you are welcome to download any branch you'd like using the `ref` input in `install_github` .

The following code block ensures the proper packages needed to run the code in this RMarkdown document are loaded. However, users may also use the `package name:: package function` notation to avoid the list of `library()` calls.

```{r}
# Load tidyverse
if(!"tidyverse"%in%installed.packages()){
  install.packages("tidyverse")
}
library(tidyverse)
```

## Help pages

All TADA R package functions have their own individual help pages, listed on the [Function reference](https://usepa.github.io/TADA/reference/index.html) page on the GitHub site. Users can also access the help page for a given function in R or RStudio using the following format (example below): `?TADA::[name of TADA function]`

```{r}
?TADA::TADAdataRetrieval
```

## Upload data

Now let's start using the TADA R package functions. The first step is to bring a dataset into the R environment. TADA is designed to work with [Water Quality Portal](https://www.waterqualitydata.us/) (WQP) data. This means that all of its functions will look for WQP column names and create new TADA-specific columns based on these elements. Users may upload their own custom dataset into R for use with TADA by ensuring their column names and data formats (e.g. numeric, character) align with WQP profiles.

If you are interested in the columns headers and formats required to run TADA, use the function below, which saves an example spreadsheet to the user's working directory. You can also take a look at an example dataset, like `TADA::Nutrients_Utah` to get an idea of the data structure and format.

```{r}
getwd() # find your working directory
TADA::getTADATemplate() # download template to working directory
```

`TADAdataRetrieval` is built upon USGS's `readWQPdata` function within the dataRetrieval package, which uses API service calls to bring WQP data into the R environment. Additionally, `TADAdataRetrieval` performs some basic quality control checks on the data using new TADA-specific columns to preserve the original dataset:

-   Converts some key character columns to ALL CAPS for easier harmonization and validation.

-   Removes complete duplicate results.

-   Identifies different classes of result values (numeric, text, percentage, comma-separated numeric, greater than/less than, numbers preceded by a tilde, etc.) and converts values to numeric where feasible.

-   Fills NA result values and units with detection limit values and units where the detection condition text indicates a censored data value.

-   Identifies censored data results and categorizes them as over-detect, non-detect, or other.

-   Unifies result and depth units to common units to improve ease of data harmonization. See `?TADA_ConvertResultUnits` and `?TADA_ConvertDepthUnits` for more information on these processes.


Let's give it a try. `TADAdataRetrieval` follows similar parameterization to `readWQPdata`, but check out the [help page](https://usepa.github.io/TADA/reference/TADAdataRetrieval.html) for all input parameters and several examples.

```{r}
# download data for NCTC's HUC12
dataset_0 = TADA::TADAdataRetrieval(
  startDate = "2020-03-14",
  endDate = "null",
  countycode = "null",
  huc = "02070004",
  siteid = "null",
  siteType = "null",
  characteristicName = "null",
  characteristicType = "null",
  sampleMedia = "null",
  statecode = "null",
  organization = "null",
  project = "null",
  applyautoclean = TRUE
)

# If TADAdataRetrieval is not up for the challenge today, uncomment and run the following:
# dataset_0 = TADA::NCTCShepherdstown_HUC12

```

```{r, echo=FALSE}
# NCTCShepherdstown_HUC12 = dataset_0
# save(NCTCShepherdstown_HUC12,file = "\\data\\NCTCShepherdstown_HUC12.Rda")
```

Currently, the `TADAdataRetrieval` function combines three WQP data profiles: Sample Results (Physical/Chemical), Site data, and Project data. This ensures that all important quality control columns are included in the dataset.

**Note:** USGS and EPA are working together to create WQP 3.0 data profiles. One data profile will contain the columns critical to TADA, removing the need to combine profiles in this first step. TADA package users likely will not notice a difference in their usage of the `TADAdataRetrieval` function, but it will simplify the steps needed to upload a custom or WQP GUI-downloaded dataset into the R package.

## Initial data review

Now that we've pulled the data into the R session, let's take a look at it. Note that any column names with a leading "TADA." were generated from the TADAdataRetrieval function.

First, always good to take a look at the data frame dimensions.

**Question 1: What are the dimensions of your dataset?**

```{r}
dim(dataset_0)
```

Before we start filtering and flagging our data, let's create a little function that performs dimension checks between the results that pass QC checks and those that do not. Dimension checks ensure we catch any situations where results may be lost or duplicated by counting the total number of rows in the input data frame, the total number of rows in the passing data frame, and the total number of rows in the removed data frame. The latter two should add up to the total number of rows of all the data.

```{r}
# defining a dimension check function that compares removed and retained data dimensions against the initial data input
dimCheck <- function(all_result_num, pass_data, fail_data, checkName){
  # check result numbers after split
final_result_num = dim(pass_data)[1] + dim(fail_data)[1]

# always good to do a dimension check
if(!all_result_num==final_result_num){
  print(paste0("Help! Results do not add up between dataset and removed after ",checkName," check."))
}else{print(paste0("Good to go. Zero results created or destroyed in ",checkName," check."))}
  
}
```

Next, we can use the `fieldCounts()` function to see how many unique values are contained within each column of the dataset. We're using the command `display = "all"` to view all of the columns but could also use `"most"` or `"key"` to cut down on the number of columns reviewed.

**Question 2: Which column should have a unique value in every row and why?**

```{r}
# different columns displayed in table
TADA::fieldCounts(dataset_0, display = "key")
TADA::fieldCounts(dataset_0, display = "all")
```

**Question 3: How many unique TADA.ActivityMediaNames exist in your dataset? Are there any media types that are not water?**

TADA is currently designed to accommodate water data from the WQP. Let's ensure that we remove all non-water data first.

```{r}
# check result numbers
all_result_num = dim(dataset_0)[1]

# remove data with media type that is not water
removed = dataset_0%>%dplyr::filter(!TADA.ActivityMediaName%in%c("WATER"))%>%dplyr::mutate(TADA.RemovalReason = "Activity media is not water.")

# what other media types exist in dataset?
unique(removed$TADA.ActivityMediaName)

# clean dataset contains only water
dataset = dataset_0%>%dplyr::filter(TADA.ActivityMediaName%in%c("WATER"))

dimCheck(all_result_num, dataset, removed, checkName = "Activity Media")

```

Two additional helper functions one can use at any step in the process are `fieldValuesTable()` and `fieldValuesPie()`. These functions create a summay table and pie chart (respectively) of all the unique values in a given column. Let's give it a try on ActivityTypeCode, which is a WQP column describing the type of sample collected for each result.

```{r}
TADA::fieldValuesTable(dataset,field="ActivityTypeCode")
TADA::fieldValuesPie(dataset,field="ActivityTypeCode")
```

**Question 4: When might a user choose to view a column's unique values as a table rather than in a pie chart?**

We can take a quick look at some of the TADA-created columns that review result value types. Because TADA is intended to work with numeric data, at this point, it would be good to remove those result values that are NA without any detection limit info, or contain text or special characters that cannot be converted to numeric. Note that TADA will fill in missing values with detection limit values and units if the ResultDetectionConditionText and DetectionQuantitationLimitType fields are populated. Use `?TADA_ConvertSpecialChars` for more details on result value types and handling.

```{r}
# take a look at datatypes
TADA::fieldValuesTable(dataset,field="TADA.ResultMeasureValueDataTypes.Flag")

# these are all of the NOT allowable data types in the dataset.
wrong_datatype = dataset%>%filter(!TADA.ResultMeasureValueDataTypes.Flag%in%c("Numeric","Less Than","Greater Than","Approximate Value","Percentage","Comma-Separated Numeric","Numeric Range - Averaged","Result Value/Unit Copied from Detection Limit"))%>%dplyr::mutate(TADA.RemovalReason = "Result value type cannot be converted to numeric or no detection limit values provided.")

# take a look at the difficult data types - do they make sense?
check = unique(wrong_datatype[,c("TADA.CharacteristicName","ResultMeasureValue","TADA.ResultMeasureValue", "ResultMeasure.MeasureUnitCode","TADA.ResultMeasure.MeasureUnitCode","TADA.ResultMeasureValueDataTypes.Flag","DetectionQuantitationLimitMeasure.MeasureValue","TADA.DetectionQuantitationLimitMeasure.MeasureValue", "DetectionQuantitationLimitMeasure.MeasureUnitCode","TADA.DetectionQuantitationLimitMeasure.MeasureUnitCode")])

dataset = dataset%>%filter(TADA.ResultMeasureValueDataTypes.Flag%in%c("Numeric","Less Than","Greater Than","Approximate Value","Percentage","Comma-Separated Numeric","Numeric Range - Averaged","Result Value/Unit Copied from Detection Limit"))

removed = plyr::rbind.fill(removed, wrong_datatype)
rm(wrong_datatype, check)

dimCheck(all_result_num, dataset, removed, checkName = "Result Format")
```

During `TADAdataRetrieval`, TADA automatically sorts result values into detection limit categories (e.g. non-detect, over-detect) based on populated values in the ResultDetectionConditionText and DetectionQuantitationLimitTypeName columns. You can find the reference tables used to make these decisions in `GetDetCondRef()` and `GetDetLimitRef()` functions. In some cases, results are missing detection limit/condition info, or there is a conflict in the detection limit and condition. The user may want to remove problematic detection limit data before proceeding.

```{r}
TADA::fieldValuesPie(dataset, field = "TADA.CensoredData.Flag")

problem_censored = dataset%>%dplyr::filter(!TADA.CensoredData.Flag%in%c("Non-Detect","Over-Detect","Other","Uncensored"))%>%dplyr::mutate(TADA.RemovalReason = "Detection limit information contains errors or missing information.")

dataset = dataset%>%dplyr::filter(TADA.CensoredData.Flag%in%c("Non-Detect","Over-Detect","Other", "Uncensored"))

removed = plyr::rbind.fill(removed, problem_censored)
rm(problem_censored)

dimCheck(all_result_num, dataset, removed, checkName = "Censored Data")
```

Let's also look at characteristics in the dataset using `dplyr` functions and pipes.

```{r}
# display full table
nrow = length(unique(dataset$TADA.CharacteristicName))

# get table of characteristics with number of results, sites, and organizations
dataset%>%dplyr::group_by(TADA.CharacteristicName)%>%dplyr::summarise(Result_Count = length(ResultIdentifier), Site_Count = length(unique(MonitoringLocationIdentifier)), Org_Count = length(unique(OrganizationIdentifier)))%>%dplyr::arrange(desc(Result_Count))%>%print(n = nrow)
```

You may see a characteristic that you'd like to investigate further in isolation. `fieldValuesPie()` will also produce summary pie charts for a given column *within* a specific characteristic. Let's take a look.

```{r}
# go ahead and pick a characteristic name from the table generated above. I picked nitrogen, mixed forms
TADA::fieldValuesPie(dataset, field = "TADA.ResultSampleFractionText", characteristicName = "NITROGEN, MIXED FORMS (NH3), (NH4), ORGANIC, (NO2) AND (NO3)")
```

Finally, we can view the site locations using a TADA mapping function. In this map, the circles indicate monitoring locations in the dataset; their size corresponds to the number of results collected at that site, while the darker the circle, the more characteristics were sampled at that site.

```{r}
TADA::TADAOverviewMap(dataset)
```

Out of curiosity, let's take a look at a breakdown of these monitoring location types. Do they all indicate surface water samples? Depending upon your program's goals and methods, you might want to filter out some of the types you see.

```{r}
TADA::fieldValuesPie(dataset, field = "MonitoringLocationTypeName")
```

## Data flagging

We've taken a quick look at the raw dataset, now let's run through some quality control checks. The most important ones to run to ensure your dataset is ready for subsequent steps are `InvalidFraction()`, `InvalidSpeciation()`, `InvalidResultUnit()`, and `QualityControlActivity()`. With the exception of `QualityControlActivity()`, these flagging functions leverage WQX's [QAQC Validation Table](https://cdx.epa.gov/wqx/download/DomainValues/QAQCCharacteristicValidation.CSV). `QualityControlActivity()` uses a TADA-specific domain table users can review with `GetActivityTypeRef()`. All QAQC tables are frequently updated in the package to ensure they match the latest version on the web. You can find guidance for using the WQX QAQC Validation Tables in this [vignette](https://usepa.github.io/TADA/articles/WQXValidationService.html) on the TADA GitHub site. 

Bring the QAQC Validation Table into your R session to view or save with the following command:

```{r}
qaqc_ref = TADA::GetWQXCharValRef()
head(qaqc_ref)
unique(qaqc_ref$Type)
```

**Question 5: What do you think the `qaqc_ref$Type` column indicates?**

TADA joins this validation table to the data and uses the "Valid" and "Invalid" labels in the Status column to create easily understandable flagging columns for each function. Let's run these four flagging functions.

```{r}
dataset_flags = TADA::InvalidFraction(dataset, clean = FALSE, errorsonly = FALSE)
dataset_flags = TADA::InvalidSpeciation(dataset_flags, clean = "none", errorsonly = FALSE)
dataset_flags = TADA::InvalidResultUnit(dataset_flags, clean = "none", errorsonly = FALSE)
dataset_flags = TADA::QualityControlActivity(dataset_flags, clean = FALSE, errorsonly = FALSE)

dimCheck(all_result_num, dataset_flags, removed, checkName = "Run Flag Functions")
```

**Question 6: Did any warnings or messages appear in the console after running these flagging functions? What do they say?**

Now that we've run all the key flagging functions, let's take a look at the results and make some decisions.

```{r}
TADA::fieldValuesPie(dataset_flags, field = "TADA.SampleFraction.Flag")
TADA::fieldValuesPie(dataset_flags, field = "TADA.MethodSpeciation.Flag")
TADA::fieldValuesPie(dataset_flags, field = "TADA.ResultUnit.Flag")
TADA::fieldValuesPie(dataset_flags, field = "TADA.ActivityType.Flag")
```

Any results flagged as "Invalid" are recognized in the QAQC Validation Table as having some data quality issue. "Nonstandardized" means that the format has not been fully vetted or processed, while "Valid" confirms that the characteristic combination is widely recognized as correctly formatted. Let's add any invalid results to the removed dataset for later review.

**Note: if you find any errors in the QAQC Validation Table, please contact the WQX Help Desk at WQX\@epa.gov to help correct it. Thanks in advance!**

```{r}
# grab all the flagged results from the four functions
problem_flagged = dataset_flags%>%filter(TADA.SampleFraction.Flag=="Invalid"|TADA.MethodSpeciation.Flag=="Invalid"|TADA.ResultUnit.Flag=="Invalid"|!TADA.ActivityType.Flag%in%("Non_QC"))%>%dplyr::mutate(TADA.RemovalReason = "Invalid Unit, Method, Speciation, or Activity Type.")

dataset_flags = dataset_flags%>%dplyr::filter(!ResultIdentifier%in%problem_flagged$ResultIdentifier)

removed = plyr::rbind.fill(removed, problem_flagged)
rm(problem_flagged)

dimCheck(all_result_num, dataset_flags, removed, checkName = "Filter Flag Functions")
```

**Question 7: Are there any other metadata columns that you review and filter in your workflow?**

We've finished running the recommended flagging functions and removing results that do not pass QC checks. Let's look at the breakdown of these data in the removed object.

```{r}
TADA::fieldValuesTable(removed, field = "TADA.RemovalReason")
```

You can review any other columns of interest and create custom domain tables of your "Valid" and "Invalid" criteria using R or Excel. Also check out some of the other flagging functions available in TADA:

-   `?AboveNationalWQXUpperThreshold()`

-   `?BelowNationalWQXLowerThreshold()`

-   `?AggregatedContinuousData()`

-   `?InvalidCoordinates()`

-   `?InvalidMethod()`

-   `?QAPPapproved()`

-   `?QAPPDocAvailable()`

-   `?idPotentialDuplicates()` - in development

Please let us know of other flagging functions you think would have broad appeal in the TADA package or need assistance brainstorming/developing.

## Censored data handling

We have already identified, flagged, and in some cases removed problematic detection limit data from our dataset, but that doesn't keep them from being difficult. Because we do not know the result value with adequate precision, water quality data users often set non-detect values to some number below the reported detection limit. TADA contains some simple methods for handling detection limits: users may multiply the detection limit by some number between 0 and 1, or convert the detection limit value to a random number between 0 and the detection limit. More complex detection limit estimation requiring regression models (Maximum Likelihood, Kaplan-Meier, Robust Regression on Order Statistics) or similar must be performed outside of the current version of TADA (though future development is planned).

```{r}
dataset_cens = TADA::simpleCensoredMethods(dataset_flags, nd_method = "multiplier",nd_multiplier = 0.5, od_method = "as-is")
```

**Question 8: How would you parameterize `simpleCensoredMethods()` to make non-detect values a random number between 0 and the provided detection limit?**

Let's take a look at how the censored data handling function affects the `TADA.ResultMeasureValueDataTypes.Flag` column.

```{r}
# before
TADA::fieldValuesTable(dataset_flags, field = "TADA.ResultMeasureValueDataTypes.Flag")
# after
TADA::fieldValuesTable(dataset_cens, field = "TADA.ResultMeasureValueDataTypes.Flag")
```

**Question 9: Is there a difference between the first and second tables?**

If you'd like to start thinking about using statistical methods to estimate detection limit values, check out the `?summarizeCensoredData` function, which accepts user-defined data groupings to prescribe estimation tests based on the number of results, % of dataset censored, and number of censoring levels (detection limits). The decision tree used in the function was outlined in an [National Nonpoint Source Tech Memo](https://www.epa.gov/sites/default/files/2016-05/documents/tech_notes_10_jun2014_r.pdf).

## Data exploration

How are you feeling about your test dataset? Does it seem ready for the next step(s) in your analyses? One of the next big steps is data harmonization: translating and aggregating synonyms, combining multiple forms/species of certain characteristics, etc. We won't get to that in this demo (more coming soon in TADA!), but we can start looking at data distributions within a single characteristic-speciation-fraction-unit using the plotting functions `TADA_hist()` and `TADA_boxplot()`. We can also view a stats table using `TADA_stats`.

Let's first take a look at the column TADA.ComparableDataIdentifier, which breaks down characteristics into groups by name, fraction, speciation, and unit. These four columns are important to evaluate together to ensure the plotted data are sufficiently similar to appear on a single plot together: it doesn't make sense to plot characteristics with different units or fractions in the same distribution.

```{r}
# trusty field values table - lets just look at the first few entries with the most associated records
TADA::fieldValuesTable(dataset_cens, field = "TADA.ComparableDataIdentifier")%>%dplyr::arrange(desc(Count))%>%head()

```

Now that we have an idea for what the TADA.ComparableDataIdentifier looks like, we can check out how it is used to plot distinct characteristic groups.

```{r}
# Look at a histogram, boxplot, and stats for TADA.ComparableDataIdentifier(s) of your choice.
comp_data_id = "TEMPERATURE, WATER_NA_NA_DEG C"
plot_data = subset(dataset_cens, dataset_cens$TADA.ComparableDataIdentifier%in%comp_data_id)
TADA::TADA_hist(plot_data, id_col = "TADA.ComparableDataIdentifier")
TADA::TADA_boxplot(plot_data, id_col = "TADA.ComparableDataIdentifier")
TADA::TADA_stats(plot_data)
```

## TADA R Shiny Modules

Finally, take a look at an alternative workflow for QC'ing WQP data: TADA Shiny Module 1: Data Discovery and Cleaning. This is a Shiny application that runs many of the TADA functions covered in this training document behind a graphical user interface. The shiny application queries the WQP, contains maps and data visualizations, flags suspect data results, handles censored data, and more. You can launch it using the code below.

```{r}
# download TADA Shiny repository
remotes::install_github("USEPA/TADAShiny", ref = "develop", dependencies = TRUE)

# launch the app locally.
TADAShiny::run_app()
```

DRAFT [Module 1](https://owshiny-dev.app.cloud.gov/tada-dev/) is also currently hosted on the web with minimal server memory/storage allocated.
