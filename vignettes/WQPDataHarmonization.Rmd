---
title: "WQP Data Harmonization"
author: "TADA Team"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{WQP Data Harmonization}
  %\usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Overview

This vignette will walk through how to discover, wrangle, and harmonize
[Water Quality Portal (WQP)](https://www.waterqualitydata.us/) data from
multiple organizations.

## Install and load packages

To install TADA, currently you need to install from GitHub using remotes
(shown) or devtools. dataRetrieval will be downloaded from CRAN, but the
development version can be downloaded directly from GitHub (un-comment).

The following code will also install any packages you do not have, and
load all packages required to run this vignette into your R session.

```{r, results = 'hide', message = FALSE, warning = FALSE}
list.of.packages <- c("plyr", "data.table", "dplyr", "magrittr", "stringr", "utils", "RColorBrewer", "stats", "gganimate", "gifski", "lubridate", "maps", "remotes", "dataRetrieval", "ggplot2")

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]

if(length(new.packages)) install.packages(new.packages)
```

If you have any issues loading the remotes library using the code above,
uncomment the line below to install the "remotes" package specifying the
repo. You must load the remotes library before installing TADA and other
packages from GitHub

```{r, results = 'hide', message = FALSE, warning = FALSE}
# install.packages("remotes", repos = "http://cran.us.r-project.org")
library(remotes)
```

Uncomment the lines below to install latest version of dataRetrieval and
ggplot2 from GitHub.

```{r, results = 'hide', message = FALSE, warning = FALSE}
# remotes::install_github("USGS-R/dataRetrieval", dependencies=TRUE)

# remotes::install_github("hadley/ggplot2", dependencies=TRUE)

# remotes::install_github("USEPA/TADA", dependencies=TRUE)
remotes::install_github("USEPA/TADA", dependencies=TRUE, ref="censored_data_eh") # EDH temporarily switched source so that this will build with new functions/names
# if you experience any issues installing TADA, try uncommenting and running the line below before the install
# options(download.file.method = "wininet")
```

Load the required libraries to run this vignette into your R session

```{r, results = 'hide', message = FALSE, warning = FALSE}
# libraries loaded from CRAN
library(plyr)
library(data.table)
library(dplyr)
library(magrittr)
library(stringr)
library(utils)
library(RColorBrewer)
library(stats)
library(rlang)
library(gganimate)
library(gifski)
library(maps)
library(lubridate)
library(ggplot2)
library(dataRetrieval)
library(TADA)
```

## Retrieve WQP data

WQP data is retrieved and processed for compatibility with TADA. This
function, **TADAdataRetrieval**, builds on the USGS dataRetrieval
package functions. It joins three WQP profiles (i.e., the station,
narrow, and phys/chem), changes all data in the Characteristic,
Speciation, Fraction, and Unit fields to uppercase, removes true
duplicates, removes data for all non-water media types, and cleans up
results with special characters.

This function uses the same inputs as the dataRetrieval `readWQPdata`
function. `readWQPdata` does not restrict the characteristics pulled
from [Water Quality Portal (WQP)](https://www.waterqualitydata.us/). You
may specify the desired characteristics by using, for instance:
characteristicName = "pH".

Data retrieval filters include:

-   statecode (review list of possible state and territory
    [abbreviations](https://www2.census.gov/geo/docs/reference/state.txt))

-   endDate

-   startDate

-   countycode

-   siteid

-   siteType

-   characteristicName

-   sampleMedia

-   project

-   organization

Please be aware that this TADAdataRetrieval function automatically runs
the TADA **autoclean** and **ConvertSpecialChars** functions
as well, which are both required for subsequent functions within the
TADA R package to run. As a general rule, TADA functions do not change any contents in the WQP-served columns (enter *?ConvertSpecialChars* and *?autoclean* into the console for more details). Instead, they add new columns with the prefix "TADA." The following columns are numeric versions of their WQP origins:

    -   TADA.ResultMeasureValue

    -   TADA.DetectionQuantitationLimitMeasure.MeasureValue
    
    -   TADA.LatitudeMeasure
    
    -   TADA.LongitudeMeasure

These functions also add the columns TADA.ResultMeasureValue.DataTypeFlag and TADA.DetectionQuantitationLimitMeasure.MeasureValue.DataTypeFlag, which provide information about the result values that is needed to address censored data later on (i.e., nondetections). Specifically, these new columns flag if special characters are included in result values, and specifies what the special characters are.

Downloads using TADAdataRetrieval will have the same columns each time,
but be aware that data are uploaded to the Water Quality Portal by
individual organizations, which may or may not follow the same
conventions. Data and metadata quality are not guaranteed! Make sure to
carefully explore any data and make conservative quality assurance
decisions where information is limited.

Tips:

1.  All the query filters for the WQP work as an AND but within the
    fields there are ORs. For example:

    -   Characteristics: If you choose pH & DO - it's an OR. This means
        you will retrieve both pH OR DO data if available.

    -   States: Similarly, if you choose VA and IL, it's an OR. This
        means you will retrieve both VA OR IL data if available.

    -   Combinations of fields are ANDs, such as State/VA AND
        Characteristic/DO". This means you will receive all DO data
        available in VA.

    -   "Characteristic" and "Characteristic Type" also work as an AND.
        This means that the Characteristic must fall within the
        CharacteristicGroup if both filters are being used, if not you
        will get an error.

2.  The "siteid" is a general term WQP uses to describe both Site IDs
    from USGS databases and Monitoring Location Identifiers (from the
    Water Quality Portal). Each monitoring location in the Water Quality
    Portal (WQP) has a unique Monitoring Location Identifier, regardless
    of the database from which it derives. The Monitoring Location
    Identifier from the WQP is the concatenated Organization Identifier
    plus the Site ID number. Site IDs that only include a number are
    only unique identifiers for monitoring locations within USGS NWIS or
    EPA's WQX databases separately.

Additional resources:

-   Review function documentation by entering the following code into
    the console: ?TADAdataRetrieval

-   [Introduction to the dataRetrieval
    package](https://CRAN.R-project.org/package=dataRetrieval)

-   [General Data Import from Water Quality
    Portal](https://rdrr.io/cran/dataRetrieval/man/readWQPdata.html)

-   [Water Quality Portal Web Services
    Guide](https://www.waterqualitydata.us/webservices_documentation/)

-   [dataRetrieval Tutorial](https://owi.usgs.gov/R/dataRetrieval.html)

## dataRetrieval

Uncomment below if you would like to review differences between the
profiles you would get using readWQPdata vs. TADAdataRetrieval. The
profiles are different because TADAdataRetrieval automatically joins in
data from multiple WQP profiles, and does some additional data cleaning
as part of the data retrieval process.

```{r}
# dataRetrieval1 <- dataRetrieval::readWQPdata(statecode = "AK", characteristicName = c("Fecal Coliform", "Escherichia coli", "Enterococcus"), startDate = "2018-05-01", ignore_attributes = TRUE)

# another example
# dataRetrieval2 <- dataRetrieval::readWQPdata(statecode = "UT", characteristicName = c("Ammonia", "Nitrate", "Nitrogen"), startDate = "2021-01-01", ignore_attributes = TRUE)

```

Use the code below to download data from the WQP using
TADAdataRetrieval. Edit the code chuck below to define your own WQP
query inputs.

```{r}
# here is an example for Utah
# TADAProfile1 <- TADAdataRetrieval(statecode = "UT", characteristicName = c("Ammonia", "Nitrate", "Nitrogen"), startDate = "2020-10-01")

# example for Alaska
# TADAProfile2 <- TADAdataRetrieval(ProjectIdentifier = "Anchorage Bacteria 20-21")

# another example query for Alaska. We will move forward with this example in the remainder of the vignette
TADAProfile <- TADAdataRetrieval(statecode = "AK", characteristicName = c("Fecal Coliform", "Escherichia coli", "Enterococcus", "Ammonia", "Nitrate", "Nitrogen"), startDate = "2018-05-01")

```

Option 2: Alternatively, you can use the data.table::fread function to
read in a web service call for any WQP profile (un-comment).

```{r}
# New_Draft_fullphyschem <- data.table::fread("https://www.waterqualitydata.us/data/Result/search?countrycode=US&statecode=US%3A49&siteid=UTAHDWQ_WQX-4925610&startDateLo=01-01-2015&startDateHi=12-31-2016&mimeType=csv&zip=no&sorted=yes&dataProfile=fullPhysChem&providers=NWIS&providers=STEWARDS&providers=STORET")
```

**Extra tip:** Note that the web service call built using the Water Quality Portal uses the inputs startDateLo and startDateHi rather than startDate and endDate, and dates are in the format MM-DD-YYYY rather than the TADAdataRetrieval and dataRetrieval format of YYYY-MM-DD. The functions use the latter format rather than the web service call date format because YYYY-MM-DD is a more easily utilized format in the R coding environment. However, users of USGS's dataRetrieval may use the date format MM-DD-YYYY *only if* they specify with "startDateLo" and "startDateHi" inputs. For coding consistency, it is recommended users stick with YYYY-MM-DD.

Option 3: If you need to download a large amount of data from across a
large area, and the TADAdataRetrieval function is not working due to WQP
timeout issues, then the **TADABigdataRetrieval** function may work
better.

This function does multiple synchronous data calls to the WQP
(waterqualitydata.us). It uses the WQP summary service to limit the
amount downloaded to only relevant data, and pulls back data from 100
stations at a time and then joins the data back together and produces a
single TADA compatible dataframe as the output. For large dataframes,
that can save a lot of time and ultimately reduce the complexity of
subsequent data processing. Using this function, you will be able to
download all data available from all sites in the contiguous United
States that is available for the time period, characteristicName,
statecode, and siteType requested.

See ?TADABigdataRetrieval for more details. WARNING, this can take
multiple HOURS to run. The total run time depends on your query inputs.

```{r}
# WARNING, this can take multiple HOURS to run 
# StreamTemp <- TADABigdataRetrieval(startDate = "2000-01-01", 
#                                     endDate = "2022-12-31", 
#                                     characteristicName = "Temperature, water",
#                                     statecode = c("AK","AL"))
# 
# WaterTemp <- TADABigdataRetrieval(characteristicName = "Temperature, water")
#
# Phosphorus <- TADABigdataRetrieval(characteristicName = "Phosphorus")
# 
# CT <- TADABigdataRetrieval(statecode = "CT")

```

Review all column names in the TADA Profile

```{r}
colnames(TADAProfile)
TADAProfile_CharSummary <- SummarizeColumn(TADAProfile,"TADA.CharacteristicName")
TADAProfile_CharSummary
```

## Invalid coordinates

Review station locations

```{r}
#create a map of the world
map()

# draw the sites included in your TADAProfile onto the map
points(TADAProfile$TADA.LongitudeMeasure, TADAProfile$TADA.LatitudeMeasure, col="red", pch=20)
```

The TADA **InvalidCoordinates** function identifies and flags invalid
coordinate data.

Allowable values for clean_outsideUSA are "no", "remove", or "change
sign". The default is "no" which flags latitude and longitude
coordinates outside the USA. Assigning clean_ousideUSA = "remove" will
remove rows of data with coordinates outside the USA. And assigning
clean_outsideUSA = "change sign" will flip the sign of latitude or
longitude coordinates flagged as outside the USA. The "change sign"
option should only be used when it is known that coordinates were
entered with the wrong sign in WQX; additionally, the data owner should
fix these incorrect coordinates in the raw data through the WQX - for
assistance email the WQX help desk: WQX\@epa.gov

Allowable values for clean_imprecise are TRUE or FALSE. The default is
FALSE which flags rows of data with invalid or imprecise coordinates.
Assigning clean_imprecise = TRUE will remove rows of data with invalid
or imprecise coordinates.

Allowable values for errorsonly are TRUE or FALSE. The default is FALSE
which keeps all rows of data regardless of flag status. Assigning
errorsonly = TRUE filters the dataframe to show only rows of data which
are flagged.

When clean_outsideUSA = "no" and/or clean_imprecise = FALSE, a column
will be appended titled "TADA.InvalidCoordinates" with the following
flags (if relevant to dataframe):

-   If the latitude is less than zero, the row will be flagged with
    "LAT_OutsideUSA". (Exception for American Samoa)

-   If the longitude is greater than zero AND less than 145, the row
    will be flagged as "LONG_OutsideUSA". (Exceptions for Guam and the
    Northern Mariana Islands)

-   If the latitude or longitude contains the string, "999", the row
    will be flagged as invalid.

-   Finally, precision can be measured by the number of decimal places
    in the latitude and longitude provided. If either does not have any
    numbers to the right of the decimal point, the row will be flagged
    as "Imprecise".

```{r}
InvalidCoordinateFlags <- InvalidCoordinates(TADAProfile, clean_outsideUSA = "no", clean_imprecise = FALSE, errorsonly = TRUE)

# review unique flags in InvalidCoordinateFlags
unique(InvalidCoordinateFlags$TADA.InvalidCoordinates)

# review unique MonitoringLocationIdentifiers in your flag dataframe
unique(InvalidCoordinateFlags$MonitoringLocationIdentifier)

Unique_InvalidCoordinateFlags <- InvalidCoordinateFlags %>%
  dplyr::select('MonitoringLocationIdentifier','MonitoringLocationName', 'TADA.InvalidCoordinates',
         'OrganizationIdentifier', 'TADA.LongitudeMeasure', 'TADA.LatitudeMeasure', 'MonitoringLocationTypeName', 
         'CountryCode', 'StateCode', 'CountyCode', 'HUCEightDigitCode', 'MonitoringLocationDescriptionText',
         'ProjectName', 'ProjectIdentifier', 'OrganizationFormalName') %>%
  dplyr::distinct()

# remove all data for sites with invalid or imprecise coordinates. Change function inputs to keep this data, but be aware it may impact your mapping. 
TADAProfileClean1 <- InvalidCoordinates(TADAProfile, clean_outsideUSA = "remove", clean_imprecise = TRUE, errorsonly = FALSE)

```

## Depth unit conversions

The **ConvertDepthUnits** function converts depth units to a consistent
unit. Depth values and units are most commonly associated with lake data, and are populated in the *ActivityDepthHeightMeasure*,
*ActivityTopDepthHeightMeasure*, *ActivityBottomDepthHeightMeasure*, and
*ResultDepthHeightMeasure* Result Value/Unit columns.

This function first checks the dataframe for depth profile data. Where depth
profile columns are populated, the function appends 'Conversion Factor'
columns and populates those columns based on the original unit and the target unit, which is defined in the 'unit' argument. A 'Depth Target Unit' column is also appended, indicating the unit all selected depth data is converted to. When
transform = FALSE, the output includes all 'Conversion Factor' columns
and the 'Depth Target Unit' column. When transform = TRUE, the output
includes converted depth data and the 'Depth Target Unit' column, which acts as a flag indicating which rows have been converted. Default is transform = TRUE.

The depth profile function can harmonize the depth units across all the
following fields (or only a specific one): "ActivityDepthHeightMeasure",
"ActivityTopDepthHeightMeasure", "ActivityBottomDepthHeightMeasure",
"ResultDepthHeightMeasure"). It creates new result value/unit columns with the prefix "TADA." to all converted columns. The default is to check all four Depth Height columns.

Allowable values for 'unit' are either 'm' (meter), 'ft' (feet), or 'in'
(inch). 'unit' accepts only one allowable value as an input. Default is
unit = "m".

See additional function documentation for additional function options by
entering the following code in the console: ?ConvertDepthUnits

```{r}
#converts all depth profile data to meters
TADAProfileClean2 <- ConvertDepthUnits(TADAProfileClean1, unit = "m", transform = TRUE)
```

## Result unit conversions

The **ConvertResultUnits** function converts all results to WQX target
units. WQX target units are pulled from the MeasureUnit domain table:

-   <https://cdx.epa.gov/wqx/download/DomainValues/MeasureUnit.CSV>

See additional function documentation for additional function options by
entering the following code in the console: ?ConvertResultUnits

```{r}
#Converts all results to WQX target units
TADAProfileClean3 <- ConvertResultUnits(TADAProfileClean2, transform = TRUE)
```

## Statistically aggregated data

The **AggregatedContinuousData** function checks for and removes
statistically aggregated high frequency (i.e., continuous) data, if
present.

The Water Quality Portal (WQP) is not currently designed to store
high-frequency sensor data (more than 1 value per day). However,
sometimes data providers choose to aggregate their continuous data to a
daily avg, max, or min value, and then submit that aggregated data to
the WQP through WQX. Alternatively, some organizations aggregate their
high frequency data (15 min or 1 hour data) to 2 or 4 hour interval
averages, and they also submit that data to the WQP through WQX. This
type of high frequency data may (or may not) be suitable for integration
with discrete water quality data for assessments. Therefore, this
function uses metadata submitted by data providers to flag rows with
aggregated continuous data. This is done by flagging results where the
ResultDetectionConditionText = "Reported in Raw Data (attached)".

-   When clean = FALSE, a column titled "TADA.AggregatedContinuousData"
    is added to the dataframe to indicate if the row includes aggregated
    continuous data, "Y", or not, "N".

-   When clean = TRUE, rows with aggregated continuous data are removed
    from the dataframe and no column will be appended. The default is
    clean = TRUE.

An additional input called errorsonly will allow the user to filter data
to show only rows of aggregated continuous data. Allowable values for
errorsonly are TRUE or FALSE. The default is FALSE which keeps all rows
of data regardless of flag status. Assigning errorsonly = TRUE filters
the dataframe to show only rows of data which are flagged "Y".

See function documentation for additional function options by entering
the following code in the console: ?AggregatedContinuousData

```{r}
TADAProfileClean4 <- AggregatedContinuousData(TADAProfileClean3, clean = TRUE)

# uncomment below to create a dataframe of only the aggregated continuous data
# TADAProfile_aggcont <- AggregatedContinuousData(TADAProfileClean3, clean = FALSE, errorsonly = TRUE)
```

## WQX QAQC Service Result Flags

Run the following result functions to address invalid method, fraction,
speciation, and unit metadata by characteristic. The default is clean =
TRUE, which will remove invalid results. You can change this to clean =
FALSE to flag results, but not remove them.

See documentation for more details:

-   ?**InvalidMethod**

    -   When clean = FALSE, this function adds the following column to
        your dataframe: WQX.AnalyticalMethodValidity. This column flags
        invalid CharacteristicName,
        ResultAnalyticalMethod/MethodIdentifier, and
        ResultAnalyticalMethod/MethodIdentifierContext combinations in
        your dataframe either "Nonstandardized", "Invalid", or "Valid".

    -   When clean = TRUE, "Invalid" rows are removed from the dataframe
        and no column will be appended.

    -   When errorsonly = TRUE, the dataframe is filtered to only the
        rows flagged as "Invalid"; default is errorsonly = FALSE.

-   ?**InvalidSpeciation**

    -   When clean = "none", this function adds the following column to
        your dataframe: WQX.MethodSpeciationValidity. This column flags
        each CharacteristicName and MethodSpecificationName combination
        in your dataframe as either "Nonstandardized", "Invalid", or
        "Valid".

    -   When clean = "invalid_only", only "Invalid" rows are removed
        from the dataframe. Default is clean = "invalid_only".

    -   When clean = "nonstandardized_only", only "Nonstandardized" rows
        are removed from the dataframe.

    -   When clean = "both", "Invalid" and "Nonstandardized" rows are
        removed from the dataframe.

    -   When clean = "none", no rows are removed from the dataframe.

    -   When errorsonly = TRUE, the dataframe is filtered to only the
        rows flagged as "Invalid" or "Nonstandardized"; default is
        errorsonly = FALSE.

-   ?**InvalidResultUnit**

    -   When clean = FALSE, the following column will be added to your
        dataframe: WQX.ResultUnitValidity. This column flags each
        CharacteristicName, sampleMedia, and
        ResultMeasure/MeasureUnitCode combination in your dataframe as
        either "Nonstandardized", "Invalid", or "Valid".

    -   When clean = TRUE, "Invalid" rows are removed from the dataframe
        and no column will be appended.

    -   When errorsonly = TRUE, the dataframe is filtered to only the
        rows flagged as "Invalid"; default is errorsonly = FALSE.

-   ?**InvalidFraction**

    -   When clean = FALSE, this function adds the following column to
        your dataframe: WQX.SampleFractionValidity. This column flags
        each CharacteristicName and ResultSampleFractionText combination
        in your dataframe as either "Nonstandardized", "Invalid", or
        "Valid".
    -   When clean = TRUE, "Invalid" rows are removed from the dataframe
        and no column will be appended.
    -   When errorsonly = TRUE, the dataframe is filtered to only the
        rows flagged as "Invalid"; default is errorsonly = FALSE.

```{r}
TADAProfileClean5 <- InvalidMethod(TADAProfileClean4, clean = TRUE)
TADAProfileClean6 <- InvalidFraction(TADAProfileClean5, clean = TRUE)
TADAProfileClean7 <- InvalidSpeciation(TADAProfileClean6, clean = "none")
TADAProfileClean8 <- InvalidResultUnit(TADAProfileClean7, clean = "none")
```

## WQX national upper and lower thresholds

Run the following code to flag or remove results that are above or below
the national upper and lower bound for each characteristic and unit
combination. See documentation for more details:

-   ?**AboveNationalWQXUpperThreshold**

    -   When clean = FALSE, the following column is added to your
        dataframe: AboveWQXUpperThreshold. This column flags rows with
        data that are above the upper WQX threshold.

    -   When clean = TRUE, data that is above the upper WQX threshold is
        removed from the dataframe. The default is clean = TRUE.

    -   When errorsonly = TRUE, the dataframe is filtered to only the
        rows flagged as above the upper WQX threshold; default is
        errorsonly = FALSE.

-   ?**BelowNationalWQXLowerThreshold**

    -   When clean = FALSE, the following column is added to your
        dataframe: BelowWQXLowerThreshold. This column flags rows with
        data that are below the lower WQX threshold.

    -   When clean = TRUE, data that is below the lower WQX threshold is
        removed from the dataframe. The default is clean = TRUE.

    -   When errorsonly = TRUE, the dataframe is filtered to only the
        rows flagged as below the lower WQX threshold; default is
        errorsonly = FALSE.

```{r}
TADAProfileClean9 <- AboveNationalWQXUpperThreshold(TADAProfileClean8, clean = TRUE)
TADAProfileClean10 <- BelowNationalWQXLowerThreshold(TADAProfileClean9, clean = TRUE)
```

## Potential duplicates

Sometimes multiple organizations submit the exact same data to Water
Quality Portal (WQP), which can affect water quality analyses and
assessments. This function checks for and identifies data that is
identical in all fields excluding organization-specific and comment text
fields. Each pair or group of potential duplicate rows is flagged with a
unique ID. For more information, review the documentation by entering
the following into the console:

-   ?**PotentialDuplicateRowID**

    -   When clean = FALSE, the following column will be added to you
        dataframe: TADA.PotentialDupRowID. This column flags potential
        duplicate rows of data in your dataframe, and assigns each
        potential duplicate combination a unique number linking the two
        potential duplication rows.

    -   When clean = TRUE, the function retains the first occurrence of
        each potential duplicate in the dataframe and removes potential
        duplicate rows. No column is appended. The default is clean =
        TRUE.

    -   When errorsonly = TRUE, the dataframe is filtered to only the
        rows flagged as potential duplicates; default is errorsonly =
        FALSE.

```{r}
TADAProfileClean11 <- PotentialDuplicateRowID(TADAProfileClean10)
```

## Review QAPP information

The **QAPPapproved** function checks data for an approved QAPP.

This function checks to see if there is any information in the column
"QAPPApprovedIndicator". Some organizations submit data for this field
to indicate if the data produced has an approved Quality Assurance
Project Plan (QAPP) or not. In this field, Y indicates yes, N indicates
no.

This function has three default inputs: clean = TRUE, cleanNA = FALSE,
and errorsonly = FALSE. These defaults remove rows of data where the
QAPPApprovedIndicator equals "N".

Users could alternatively remove both N's and NA's using the inputs
clean = TRUE, cleanNA = TRUE, and errorsonly = FALSE.

Additionally, users could filter to show only N's and NA's by using the
inputs clean = FALSE, cleanNA = FALSE, and errorsonly = TRUE.

If clean = FALSE, cleanNA = FALSE, and errorsonly = FALSE, the function
will not do anything.

```{r}
TADAProfileClean12 <- QAPPapproved(TADAProfileClean11, clean = TRUE, cleanNA = FALSE)
```

The **QAPPDocAvailable** function checks to see if a QAPP Doc is
Available

This function checks data submitted under the "ProjectFileUrl" column to
determine if a QAPP document is available to review. When clean = FALSE,
a column will be appended to flag results that do have an associated
QAPP document URL provided. When clean = TRUE, rows that do not have an
associated QAPP document are removed from the dataframe and no column
will be appended. When errorsonly = TRUE, the dataframe is filtered to
show only rows that do not have an associated QAPP document. The
defaults are clean = FALSE and errorsonly = FALSE. This function should
only be used to remove data if an accompanying QAPP document is required
to use data in assessments.

```{r}
TADAProfileClean13 <- QAPPDocAvailable(TADAProfileClean12, clean = FALSE)
```

## Filter data by field

In this section a TADA user will want to review the unique values in
specific fields and may choose to remove data with particular values.

To start, review the list of fields and the number of unique values in
each field using the **FilterFields** function.

```{r}
FilterFields(TADAProfileClean13)
```

Next, choose a field from the list to see the unique values in that
field, as well as the number of times each value appears in the
dataframe using the **FilterFieldReview** function. We'll start with
ActivityTypeCode.

Here is a list of other fields to review:

1.  **ResultCommentText** often has details relating to additional QA.

2.  **MeasureQualifierCode** Contains information about data flags

3.  

        Other codes may designate suspect data or other flags which may be described in detail in **ResultLaboratoryCommentText** or another column

```{r, fig.width=6, fig.height=2, fig.fullwidth=TRUE}
FilterFieldReview("ActivityTypeCode", TADAProfileClean13)
```

The ActivityTypeCode field has four unique values -- "Sample-Routine",
"Quality Control Sample-Field Replicate", "Field Msr/Obs", and "Quality
Control Sample-Field Blank." In this example we want to remove quality
control values in the ActivityTypeCode field, therefore, we'll specify
that we want to remove the "Quality Control Sample-Field Replicate" and
"Quality Control Sample-Field Blank" values in the ActivityTypeCode
field.

```{r}
TADAProfileClean14 <- dplyr::filter(TADAProfileClean13, !(ActivityTypeCode %in% c("Quality Control Sample-Field Replicate", "Quality Control Sample-Field Blank", "Quality Control Sample-Lab Duplicate", "Quality Control Sample-Equipment Blank")))
```

We've completed our review of the ActivityTypeCode field.

Let's move on to a different field and see if there are any values that
we want to remove -- we'll look at the values in the
ResultStatusIdentifier field.

```{r, fig.width=6, fig.height=2, fig.fullwidth=TRUE}
FilterFieldReview("ActivityMediaSubdivisionName", TADAProfileClean14)
```

The ActivityMediaSubdivisionName field has two unique values, "Surface
Water" and "Groundwater." In this example we want to remove the
"Groundwater" values.

```{r}
TADAProfileClean15 <- dplyr::filter(TADAProfileClean14, !(ActivityMediaSubdivisionName %in% c("Groundwater", "Bulk deposition")))
```

## Filter data by field, subset by parameter

In this section a TADA user will want to select a parameter, review the
unique values associated with that parameter in specific fields, and
choose to remove particular values.

To start, review the list of parameters in the dataframe using the
**FilterParList** function. (The list is sorted from highest to lowest
counts. Only the first few rows are displayed to save space on the page)

```{r}
FilterParList(TADAProfileClean15)
```

Next, select a parameter. Let's explore the fields associated with
Nitrogen using the **FilterParFields** function:

```{r}
FilterParFields(TADAProfileClean15, "NITROGEN")
```

Selecting a parameter generates the list below, which is subset by the
selected parameter, of fields and the number of unique values in each
field.

Then choose a field from the list. In this example we'll remove certain
values from the HydrologicEvent field.

```{r, fig.width=6, fig.height=2, fig.fullwidth=TRUE}
FilterParFieldReview("HydrologicEvent", TADAProfileClean15, "NITROGEN")
```

The HydrologicEvent field has three unique values. In this example we
want to remove samples collected during "Storm" events. Therefore, we'll
specify that we want to remove rows where the CharacteristicName is
"NITROGEN" and the HydrologicEvent field is "Storm."

```{r}
TADAProfileClean16 <- dplyr::filter(TADAProfileClean15, !(CharacteristicName %in% "NITROGEN" & HydrologicEvent %in% "Storm"))
```

## Transform Characteristic, Speciation, and Unit values to TADA Standards

The **HarmonizeRefTable** function generates a harmonization reference
table that is specific to the input dataframe. Users can review how
their input data relates to standard TADA values for the following
elements:

-   TADA.CharacteristicName

-   TADA.ResultSampleFractionText

-   TADA.MethodSpecificationName

-   TADA.ResultMeasure.MeasureUnitCode

The **HarmonizeData** function then compares the input dataframe to the
TADA Harmonization Reference Table. The purpose of the function is to
make similar data consistent and therefore easier to compare and
analyze.

Users can also edit the reference file to meet their needs if desired.
The download argument can be used to save the harmonization file to your
current working directory when download = TRUE, the default is download
= FALSE.

Optional outputs include:

1.  the dataframe with Harmonization columns appended,

2.  the dataframe with TADA.CharacteristicName, TADA.ResultSampleFractionText,
    TADA.MethodSpecificationName, and TADA.ResultMeasure.MeasureUnitCode converted
    to TADA standards or

3.  the four fields converted with most Harmonization Reference Table
    columns appended. Default is transform = TRUE and flag = TRUE.

Here are some examples of how the HarmonizeData function can be used:

1.  TADA.ResultSampleFractionText specifies forms of constituents. In some
    cases, a single \*CharacteristicName\*\* will have both "Total" and
    "Dissolved" forms specified, which should not be combined. In these
    cases, each TADA.CharacteristicName and TADA.ResultSampleFractionText
    combination is given a different identifier. This identifier can be
    used later on to identify comparable data groups for calculating
    statistics and creating figures for each combination.

2.  Some variables have different names but represent the same
    constituent (e.g., "Total Kjeldahl nitrogen (Organic N & NH3)" and
    "Kjeldahl nitrogen"). The HarmonizeData function gives a consistent
    name (and identifier) to synonyms.

```{r}
UniqueHarmonizationRef <- HarmonizationRefTable(TADAProfileClean16, download = FALSE)
TADAProfileClean17 <- HarmonizeData(TADAProfileClean16, ref = UniqueHarmonizationRef, transform = TRUE, flag = TRUE)
```

## Characteristic Specific Analyses Below

Address censored data for a single characteristic.

```{r}
# how to point to a column in R: yourdataset$thecolumnyouwant
# e.g., temp$ResultMeasureValueNitrogenExample <- filter(TADAProfileClean17, CharacteristicName == "NITROGEN")
SingleCharacteristic <- dplyr::filter(TADAProfileClean17, TADA.CharacteristicName == "NITROGEN")

# review the types of censored data you have in your data frame
table(SingleCharacteristic$ResultDetectionConditionText)

# review number of results that are NA's
sum(is.na(SingleCharacteristic$TADA.ResultMeasureValue))

# substitute a value for NA. In this example, 0.02 MG/L is used for NITROGEN
SingleCharacteristic$TADA.ResultMeasureValue <- ifelse(is.na(SingleCharacteristic$TADA.ResultMeasureValue) == TRUE, 0.02, SingleCharacteristic$TADA.ResultMeasureValue)

```

Summarize results for a single characteristic

```{r}
# add na.rm = TRUE to the function above to strip NA values before the mean computation proceeds
summary(SingleCharacteristic$TADA.ResultMeasureValue, na.rm = TRUE)

hist(SingleCharacteristic$TADA.ResultMeasureValue, main = unique(SingleCharacteristic$TADA.CharacteristicName), xlab = "Values")

```
