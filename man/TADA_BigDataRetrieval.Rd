% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DataDiscoveryRetrieval.R
\name{TADA_BigDataRetrieval}
\alias{TADA_BigDataRetrieval}
\title{Large WQP data pulls using dataRetrieval}
\usage{
TADA_BigDataRetrieval(
  startDate = "null",
  endDate = "null",
  huc = "null",
  siteid = "null",
  siteType = "null",
  characteristicName = "null",
  characteristicType = "null",
  sampleMedia = "Water",
  statecode = "null",
  maxrecs = 250000,
  applyautoclean = FALSE
)
}
\arguments{
\item{startDate}{Start Date string in the format YYYY-MM-DD, for example, "2020-01-01"}

\item{endDate}{End Date string in the format YYYY-MM-DD, for example, "2020-01-01"}

\item{huc}{A numeric code denoting a hydrologic unit. Example: "04030202". Different size hucs can be entered.}

\item{siteid}{Unique monitoring station identifier}

\item{siteType}{Type of waterbody}

\item{characteristicName}{Name of parameter}

\item{characteristicType}{Groups of environmental measurements/parameters.}

\item{sampleMedia}{Sampling substrate such as water, air, or sediment}

\item{statecode}{Code that identifies a state}

\item{maxrecs}{The maximum number of results queried within one call to dataRetrieval.}

\item{applyautoclean}{Logical, defaults to TRUE. Applies TADA_AutoClean function on the returned data profile.}
}
\value{
TADA-compatible dataframe
}
\description{
This function does multiple synchronous data calls to the WQP
(waterqualitydata.us). It uses the WQP summary service to limit the amount
downloaded to only relevant data (based on user query), pulls back data for
250000 records at a time, and then joins the data back together to produce a
single TADA compatible dataframe as the output. For large data sets, that can save a lot
of time and ultimately reduce the complexity of subsequent data processing.
Using this function, you will be able to download all data available from all
sites in the contiguous United States available for the time period,
characteristicName, and siteType requested. Computer memory may limit the
size of datasets that your R console will be able to hold in one session.
Function requires a characteristicName, siteType, statecode, huc, or start/
end date input. The recommendation is to be as specific as you can with your
large data call. The function allows the user to run TADA_AutoClean on the dataset,
but this is not the default as checking large dataframes for exact duplicate
rows can be time consuming and is better performed on its own once the query is
completed.
}
\details{
Some code for this function was adapted from this USGS Blog (Author: Aliesha Krall)
\href{https://waterdata.usgs.gov/blog/large_sample_pull/}{Large Sample Pull}

See ?TADA_AutoClean documentation for more information on this optional input.

Note: TADA_BigDataRetrieval (by leveraging USGS's dataRetrieval),  automatically converts
the date times to UTC. It also automatically converts the data to dates,
datetimes, numerics based on a standard algorithm. See: ?dataRetrieval::readWQPdata
}
\examples{
\dontrun{
tada1 <- TADA_BigDataRetrieval(startDate = "2019-01-01", endDate = "2021-12-31", characteristicName = "Temperature, water", statecode = c("AK", "AL"))
tada2 <- TADA_BigDataRetrieval(startDate = "2016-10-01", endDate = "2022-09-30", statecode = "UT")
tada3 <- TADA_BigDataRetrieval(huc = "04030202", characteristicName = "Escherichia coli")
tada4 <- TADA_BigDataRetrieval(huc = c("04030202", "04030201"), characteristicName = "Temperature, water")
}

}
